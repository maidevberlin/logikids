# AI Configuration Template
# Copy this file to config.yaml and adjust the values
server:
  port: 3000
ai:
  # Available providers: 'ollama', 'openai', or 'anthropic'
  provider: ollama

    # Ollama Configuration (required if provider is 'ollama')
    ollama:
      host: http://host.docker.internal:11434
      model: llama2
      temperature: 0.7
      top_k: 40
      top_p: 0.9

    # OpenAI Configuration (required if provider is 'openai')
    openai:
      apiKey: your-api-key-here
      model: gpt-5
      temperature: 0.7
      maxTokens: 2000
      topP: 0.9
    # Available Models (2025):
    # - gpt-5: Most advanced, best for complex educational content (Released Aug 2025)
    # - gpt-5-mini: Fast and affordable, recommended for most use cases
    # - gpt-5-nano: Fastest and most affordable reasoning model
    # - gpt-4.1: Improved instruction-following, cost-effective alternative (Released Apr 2025)
    # - gpt-4.1-mini: Good balance of performance and cost
    # - gpt-4.1-nano: Most affordable option

    # Anthropic Configuration (required if provider is 'anthropic')
    anthropic:
      apiKey: your-anthropic-api-key-here
      model: claude-3-5-sonnet-20241022
      maxTokens: 4096
      temperature: 0.7
    # Available Models:
    # - claude-3-5-sonnet-20241022: Best balance of intelligence and speed (Recommended)
    # - claude-3-5-haiku-20241022: Fastest model, great for simple tasks
    # - claude-3-opus-20240229: Most capable, slower but highest quality

  image:
    # Available providers: 'ollama' or 'openai'
    provider: ollama

    # Ollama Configuration (required if provider is 'ollama')
    ollama:
      host: http://host.docker.internal:11434
      model: llama3.2-vision
      temperature: 0.7
      top_k: 40
      top_p: 0.9

    # OpenAI Configuration (required if provider is 'openai')
    openai:
      apiKey: your-api-key-here
      model: dall-e-3
      size: "1024x1024"
      quality: "standard"
      style: "natural" 
